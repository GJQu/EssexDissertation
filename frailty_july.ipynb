{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successful Attempt of Frailty Index\n",
    "#### By Gavin Qu, July 7th 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "- Reads each wave's data from its respective file\n",
    "- Extracts only the specified variables for each wave\n",
    "- Adds a 'wave' column to identify the source wave for each row\n",
    "- Combines all waves' data into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave a, extracted 29 variables\n",
      "Processed wave b, extracted 29 variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_96863/1201396102.py:33: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  available_columns = stata_file.variable_labels().keys()\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_96863/1201396102.py:39: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave c, extracted 45 variables\n",
      "Processed wave d, extracted 45 variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_96863/1201396102.py:33: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  available_columns = stata_file.variable_labels().keys()\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_96863/1201396102.py:39: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave e, extracted 45 variables\n",
      "Processed wave f, extracted 45 variables\n",
      "Processed wave g, extracted 45 variables\n",
      "Processed wave h, extracted 45 variables\n",
      "Processed wave i, extracted 45 variables\n",
      "Processed wave j, extracted 43 variables\n",
      "Processed wave k, extracted 43 variables\n",
      "Processed wave l, extracted 43 variables\n",
      "Processed wave m, extracted 43 variables\n",
      "Extracted data saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_extracted.csv\n",
      "Total rows: 533476\n",
      "Total columns: 534\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set directory\n",
    "data_dir = '/Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls'\n",
    "output_dir = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data'\n",
    "\n",
    "# Base list of relevant variables (without wave prefix)\n",
    "base_variables = [\n",
    "    'pidp',\n",
    "    'age_dv',\n",
    "    'disdif1', 'disdif2', 'disdif3', 'disdif4', 'disdif5', 'disdif6', 'disdif7', 'disdif8',\n",
    "    'disdif9', 'disdif10', 'disdif11',\n",
    "    'hcond1', 'hcond2', 'hcond3', 'hcond4', 'hcond5', 'hcond6', 'hcond7', 'hcond8', \n",
    "    'hcond9', 'hcond10', 'hcond11', 'hcond12', 'hcond13', 'hcond14', 'hcond15', 'hcond16', \n",
    "    'hcondn1', 'hcondn2', 'hcondn3', 'hcondn4', 'hcondn5', 'hcondn6', 'hcondn7', 'hcondn8', \n",
    "    'hcondn9', 'hcondn10', 'hcondn11', 'hcondn12', 'hcondn13', 'hcondn14', 'hcondn15', 'hcondn16', \n",
    "    'hcondever1', 'hcondever2', 'hcondever3', 'hcondever4', 'hcondever5', 'hcondever6', 'hcondever7', 'hcondever8', \n",
    "    'hcondever9', 'hcondever10', 'hcondever11', 'hcondever12', 'hcondever13', 'hcondever14', 'hcondever15', 'hcondever16', \n",
    "    'hcondnew1', 'hcondnew2', 'hcondnew3', 'hcondnew4', 'hcondnew5', 'hcondnew6', 'hcondnew7', 'hcondnew8', \n",
    "    'hcondnew9', 'hcondnew10', 'hcondnew11', 'hcondnew12', 'hcondnew13', 'hcondnew14', 'hcondnew15', 'hcondnew16', \n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Process each wave\n",
    "for wave in 'abcdefghijklm':\n",
    "    file_path = os.path.join(data_dir, f'{wave}_indresp.dta')\n",
    "    \n",
    "    # Read the .dta file to get available columns\n",
    "    with pd.read_stata(file_path, iterator=True) as stata_file:\n",
    "        available_columns = stata_file.variable_labels().keys()\n",
    "    \n",
    "    # Create a list of variables that exist in this wave's data\n",
    "    wave_vars = ['pidp'] + [f'{wave}_{var}' for var in base_variables[1:] if f'{wave}_{var}' in available_columns]\n",
    "    \n",
    "    # Read only the available columns\n",
    "    df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n",
    "    df_wave['wave'] = wave\n",
    "    \n",
    "    df_list.append(df_wave)\n",
    "    print(f\"Processed wave {wave}, extracted {len(wave_vars)} variables\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'ukhls_extracted.csv')\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Extracted data saved to {output_path}\")\n",
    "print(f\"Total rows: {len(df_combined)}\")\n",
    "print(f\"Total columns: {len(df_combined.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling and New Variable Construction\n",
    "#### Creating the 'healthcond' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in the next script using the 'ukhls_extracted.csv' we just created, I want to create new variables '{prefix}_healthcond1' to '{prefix}_healthcond16' for all 13 waves. \n",
    "I'm doing this as a way to track chronic illnesses overtime, since 'hcond', 'hcondn', 'hcondever', 'hcondnew' are all tracked differently. For example, 'hcond' tracks chronic illness status of those new participants in the dataset, so wave a of 'hcond' tracks all new participants, while wave c only tracks the newly entered participants. Where 'hcondn' and 'hcondnew' only tracks the existing participants from certain waves. and some of those waves of hcondn and hcondnew don't exist because both of them together forms the complete picture. \n",
    "And 'hcondever' tracks all participants again at wave 10 only, so it's asking all existing participants up to wave 10 whether they are diagnosed with any of the 16 chronic illnesses, an answer of 0 is no, 1 is yes, negative 1 to negative 9 are various forms of missing data. \n",
    "'hcond' and 'hcondever' ask individuals if they have ever been diagnosed with a specific illness, while 'hcondn' and 'hcondnew' ask if any new illness have been diagnosed. So hcond from wave 2 and on will have lots of negative values since it only tracks new participants answer as 1 and 0. While 'hcondn' and 'hcondnew' and 'hcondever' will ask all existing ones if a new condition as been diagnosed. \n",
    "\n",
    "The new variables I want to create is called {prefix}_healthcond_1 to 16, that basically tracks a positive diagnoses as 1, and tracks a negative diagnoses as 0, but since we're talking about chronic illness, once an pidp answers a 1 in any wave, any wave forward will carry that 1. While a 0 value should only be there if current and all previous wave of that health condition variable is 0. And it should be NaN if all previous and current wave is negative number.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_extracted.csv')\n",
    "\n",
    "# Define waves and health condition variables\n",
    "waves = 'abcdefghijklm'\n",
    "health_vars = ['hcond', 'hcondn', 'hcondever', 'hcondnew']\n",
    "\n",
    "# Create a list of all possible health condition columns\n",
    "all_health_cols = [f'{w}_{v}{i}' for w in waves for v in health_vars for i in range(1, 17)]\n",
    "\n",
    "# Filter to only include columns that exist in the dataframe\n",
    "health_cols = [col for col in all_health_cols if col in df.columns]\n",
    "\n",
    "# Create a mask for positive diagnoses (1), negative diagnoses (0), and missing data\n",
    "positive_mask = df[health_cols] == 1\n",
    "negative_mask = (df[health_cols] == 0) | (df[health_cols] < 0)\n",
    "missing_mask = df[health_cols].isna()\n",
    "\n",
    "# Create new health condition variables\n",
    "for i in range(1, 17):\n",
    "    cols_i = [col for col in health_cols if col.endswith(str(i))]\n",
    "    df[f'healthcond_{i}'] = np.select(\n",
    "        [positive_mask[cols_i].any(axis=1),\n",
    "         negative_mask[cols_i].all(axis=1) & ~missing_mask[cols_i].all(axis=1)],\n",
    "        [1, 0],\n",
    "        default=np.nan\n",
    "    )\n",
    "\n",
    "# Forward fill the 1s for each pidp\n",
    "df = df.sort_values(['pidp', 'wave'])\n",
    "for i in range(1, 17):\n",
    "    df[f'healthcond_{i}'] = df.groupby('pidp')[f'healthcond_{i}'].transform(\n",
    "        lambda x: x.replace({0: np.nan}).ffill().fillna(0))\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv('ukhls_health_conditions_vectorized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test count of the healthcond variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Total    Ones   Zeros  NaNs  Total_percent  Ones_percent  \\\n",
      "healthcond_1   533476   72236  461240     0          100.0         13.54   \n",
      "healthcond_2   533476   89435  444041     0          100.0         16.76   \n",
      "healthcond_3   533476   29687  503789     0          100.0          5.56   \n",
      "healthcond_4   533476   44129  489347     0          100.0          8.27   \n",
      "healthcond_5   533476   19256  514220     0          100.0          3.61   \n",
      "healthcond_6   533476  106461  427015     0          100.0         19.96   \n",
      "healthcond_7   533476   10312  523164     0          100.0          1.93   \n",
      "healthcond_8   533476    4186  529290     0          100.0          0.78   \n",
      "healthcond_9   533476    6133  527343     0          100.0          1.15   \n",
      "healthcond_10  533476   19378  514098     0          100.0          3.63   \n",
      "healthcond_11  533476    9672  523804     0          100.0          1.81   \n",
      "healthcond_12  533476   10158  523318     0          100.0          1.90   \n",
      "healthcond_13  533476   26378  507098     0          100.0          4.94   \n",
      "healthcond_14  533476   36020  497456     0          100.0          6.75   \n",
      "healthcond_15  533476    5674  527802     0          100.0          1.06   \n",
      "healthcond_16  533476  100244  433232     0          100.0         18.79   \n",
      "\n",
      "               Zeros_percent  NaNs_percent  \n",
      "healthcond_1           86.46           0.0  \n",
      "healthcond_2           83.24           0.0  \n",
      "healthcond_3           94.44           0.0  \n",
      "healthcond_4           91.73           0.0  \n",
      "healthcond_5           96.39           0.0  \n",
      "healthcond_6           80.04           0.0  \n",
      "healthcond_7           98.07           0.0  \n",
      "healthcond_8           99.22           0.0  \n",
      "healthcond_9           98.85           0.0  \n",
      "healthcond_10          96.37           0.0  \n",
      "healthcond_11          98.19           0.0  \n",
      "healthcond_12          98.10           0.0  \n",
      "healthcond_13          95.06           0.0  \n",
      "healthcond_14          93.25           0.0  \n",
      "healthcond_15          98.94           0.0  \n",
      "healthcond_16          81.21           0.0  \n",
      "\n",
      "No unexpected values found. All values are 0, 1, or NaN as expected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_health_conditions_vectorized.csv')\n",
    "\n",
    "# Function to count value types\n",
    "def count_value_types(series):\n",
    "    return pd.Series({\n",
    "        'Total': len(series),\n",
    "        'Ones': (series == 1).sum(),\n",
    "        'Zeros': (series == 0).sum(),\n",
    "        'NaNs': series.isna().sum()\n",
    "    })\n",
    "\n",
    "# Analyze healthcond variables\n",
    "healthcond_vars = [f'healthcond_{i}' for i in range(1, 17)]\n",
    "results = df[healthcond_vars].apply(count_value_types).T\n",
    "\n",
    "# Calculate percentages\n",
    "results_percent = results.div(results['Total'], axis=0) * 100\n",
    "\n",
    "# Combine counts and percentages\n",
    "final_results = pd.concat([results, results_percent.add_suffix('_percent')], axis=1)\n",
    "\n",
    "# Display results\n",
    "print(final_results.round(2))\n",
    "\n",
    "# Optional: Save results to CSV\n",
    "final_results.round(2).to_csv('healthcond_analysis.csv')\n",
    "\n",
    "# Additional check for any unexpected values\n",
    "unexpected_values = df[healthcond_vars].apply(lambda x: x[(x != 1) & (x != 0) & (~x.isna())].value_counts())\n",
    "if unexpected_values.sum().sum() > 0:\n",
    "    print(\"\\nUnexpected values found:\")\n",
    "    print(unexpected_values)\n",
    "else:\n",
    "    print(\"\\nNo unexpected values found. All values are 0, 1, or NaN as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the Frailty Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the main dataset\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_health_conditions_vectorized.csv')\n",
    "\n",
    "# Load the death dataset\n",
    "death_df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/extracted_dcsedw_dv_with_deaths.csv')\n",
    "\n",
    "# Define waves\n",
    "waves = 'abcdefghijklm'\n",
    "\n",
    "# Create frailty variables\n",
    "for wave in waves:\n",
    "    # Identify relevant columns for this wave\n",
    "    healthcond_cols = [f'healthcond_{i}' for i in range(1, 17)]\n",
    "    disdif_cols = [f'{wave}_disdif{i}' for i in range(1, 12)]\n",
    "    \n",
    "    # Count 1s in healthcond and disdif variables\n",
    "    df[f'{wave}_frailty'] = df[healthcond_cols + disdif_cols].apply(lambda row: \n",
    "        (row == 1).sum() / len(row), axis=1)\n",
    "\n",
    "# Sort the dataframe by pidp and wave\n",
    "df = df.sort_values(['pidp', 'wave'])\n",
    "\n",
    "# Create a death indicator\n",
    "death_indicator = death_df.set_index('pidp')[['b_death', 'c_death', 'd_death', 'e_death', \n",
    "                                              'f_death', 'g_death', 'h_death', 'i_death', \n",
    "                                              'j_death', 'k_death', 'l_death', 'm_death']]\n",
    "\n",
    "# Merge death indicator with main dataframe\n",
    "df = df.merge(death_indicator, on='pidp', how='left')\n",
    "\n",
    "# Update frailty based on death\n",
    "for i, wave in enumerate(waves[1:], start=1):  # Start from 'b' as there's no 'a_death'\n",
    "    death_col = f'{wave}_death'\n",
    "    df.loc[df[death_col] == 1, [f'{w}_frailty' for w in waves[i:]]] = 1\n",
    "\n",
    "# Fill NaN values in frailty with 0\n",
    "frailty_cols = [f'{wave}_frailty' for wave in waves]\n",
    "df[frailty_cols] = df[frailty_cols].fillna(0)\n",
    "\n",
    "# Save the result\n",
    "output_path = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_with_frailty.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Data with frailty measures saved to {output_path}\")\n",
    "\n",
    "# Quick summary of frailty measures\n",
    "frailty_summary = df[frailty_cols].describe()\n",
    "print(\"\\nFrailty Summary:\")\n",
    "print(frailty_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
