{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Frailty Index\n",
    "#### By Gavin Qu, July 10th 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "- Reads each wave's data from its respective file\n",
    "- Extracts only the specified variables for each wave\n",
    "- Adds a 'wave' column to identify the source wave for each row\n",
    "- Combines all waves' data into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave a, extracted 29 variables\n",
      "Processed wave b, extracted 29 variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/1236701905.py:33: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  available_columns = stata_file.variable_labels().keys()\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/1236701905.py:39: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave c, extracted 45 variables\n",
      "Processed wave d, extracted 45 variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/1236701905.py:33: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  available_columns = stata_file.variable_labels().keys()\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/1236701905.py:39: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed wave e, extracted 45 variables\n",
      "Processed wave f, extracted 45 variables\n",
      "Processed wave g, extracted 45 variables\n",
      "Processed wave h, extracted 45 variables\n",
      "Processed wave i, extracted 45 variables\n",
      "Processed wave j, extracted 43 variables\n",
      "Processed wave k, extracted 43 variables\n",
      "Processed wave l, extracted 43 variables\n",
      "Processed wave m, extracted 43 variables\n",
      "Extracted data saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_extracted.csv\n",
      "Total rows: 533476\n",
      "Total columns: 76\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set directory\n",
    "data_dir = '/Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls'\n",
    "output_dir = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data'\n",
    "\n",
    "# Base list of relevant variables (without wave prefix)\n",
    "base_variables = [\n",
    "    'pidp',\n",
    "    'age_dv',\n",
    "    'disdif1', 'disdif2', 'disdif3', 'disdif4', 'disdif5', 'disdif6', 'disdif7', 'disdif8',\n",
    "    'disdif9', 'disdif10', 'disdif11',\n",
    "    'hcond1', 'hcond2', 'hcond3', 'hcond4', 'hcond5', 'hcond6', 'hcond7', 'hcond8', \n",
    "    'hcond9', 'hcond10', 'hcond11', 'hcond12', 'hcond13', 'hcond14', 'hcond15', 'hcond16', \n",
    "    'hcondn1', 'hcondn2', 'hcondn3', 'hcondn4', 'hcondn5', 'hcondn6', 'hcondn7', 'hcondn8', \n",
    "    'hcondn9', 'hcondn10', 'hcondn11', 'hcondn12', 'hcondn13', 'hcondn14', 'hcondn15', 'hcondn16', \n",
    "    'hcondever1', 'hcondever2', 'hcondever3', 'hcondever4', 'hcondever5', 'hcondever6', 'hcondever7', 'hcondever8', \n",
    "    'hcondever9', 'hcondever10', 'hcondever11', 'hcondever12', 'hcondever13', 'hcondever14', 'hcondever15', 'hcondever16', \n",
    "    'hcondnew1', 'hcondnew2', 'hcondnew3', 'hcondnew4', 'hcondnew5', 'hcondnew6', 'hcondnew7', 'hcondnew8', \n",
    "    'hcondnew9', 'hcondnew10', 'hcondnew11', 'hcondnew12', 'hcondnew13', 'hcondnew14', 'hcondnew15', 'hcondnew16', \n",
    "]\n",
    "\n",
    "# Initialize an empty list to store DataFrames variables \n",
    "df_list = []\n",
    "\n",
    "# Process each wave\n",
    "for wave in 'abcdefghijklm':\n",
    "    file_path = os.path.join(data_dir, f'{wave}_indresp.dta')\n",
    "    \n",
    "    # Read the .dta file to get available columns\n",
    "    with pd.read_stata(file_path, iterator=True) as stata_file:\n",
    "        available_columns = stata_file.variable_labels().keys()\n",
    "    \n",
    "    # Create a list of variables that exist in this wave's data\n",
    "    wave_vars = ['pidp'] + [f'{wave}_{var}' for var in base_variables[1:] if f'{wave}_{var}' in available_columns]\n",
    "    \n",
    "    # Read only the available columns\n",
    "    df_wave = pd.read_stata(file_path, columns=wave_vars, convert_categoricals=False)\n",
    "    df_wave['wave'] = wave\n",
    "    \n",
    "    # Remove wave prefix from column names\n",
    "    df_wave.columns = ['pidp' if col == 'pidp' else col[2:] if col.startswith(f'{wave}_') else col for col in df_wave.columns]\n",
    "    \n",
    "    df_list.append(df_wave)\n",
    "    print(f\"Processed wave {wave}, extracted {len(wave_vars)} variables\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, 'ukhls_extracted.csv')\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Extracted data saved to {output_path}\")\n",
    "print(f\"Total rows: {len(df_combined)}\")\n",
    "print(f\"Total columns: {len(df_combined.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort dataframe by pidp and wave\n",
    "**The resulting csv file from the previous step does not organize the 'pidp' correctly and next to each other, so the following script with reorganize it into a easier to work with long panel with the same 'pidp' all below each other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long panel format data saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_long_panel_format.csv\n",
      "Total rows: 533476\n",
      "Total columns: 76\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set input and output directories\n",
    "input_dir = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data'\n",
    "output_dir = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data'\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file = os.path.join(input_dir, 'ukhls_extracted.csv')\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Sort the dataframe by 'pidp' and 'wave'\n",
    "df_sorted = df.sort_values(['pidp', 'wave'])\n",
    "\n",
    "# Reset the index\n",
    "df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "# Save the sorted dataframe to a new CSV file\n",
    "output_file = os.path.join(output_dir, 'ukhls_long_panel_format.csv')\n",
    "df_sorted.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Long panel format data saved to {output_file}\")\n",
    "print(f\"Total rows: {len(df_sorted)}\")\n",
    "print(f\"Total columns: {len(df_sorted.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling and New Variable Construction\n",
    "#### Creating the 'healthcond' variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For wave 'a', it only considers hcond.\n",
    "2. It skips wave 'b' for hcond.\n",
    "3. For waves 'c' through 'i' (waves 2-9), it considers both hcond (for new participants) and hcondn (for existing participants).\n",
    "4. For wave 'j' (wave 10), it uses hcond and hcondever.\n",
    "5. For waves 'k', 'l', and 'm' (waves 11-13), it uses hcond and hcondnew.\n",
    "6. It converts all negative values to NaN.\n",
    "7. After processing each wave, it forward-fills the health condition status for each individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3273271964.py:51: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_health_conditions_long_panel.csv\n",
      "Total rows: 533476\n",
      "Total columns: 92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_long_panel_format.csv')\n",
    "\n",
    "# Convert all negative values to NaN\n",
    "df = df.apply(lambda x: x.where(x >= 0, np.nan) if x.dtype.kind in 'biufc' else x)\n",
    "\n",
    "# Define waves\n",
    "waves = 'abcdefghijklm'\n",
    "\n",
    "def create_health_condition_columns(df):\n",
    "    for i in range(1, 17):\n",
    "        condition_col = f'healthcond{i}'\n",
    "        df[condition_col] = np.nan\n",
    "        \n",
    "        hcond_col = f'hcond{i}'\n",
    "        hcondn_col = f'hcondn{i}'\n",
    "        hcondever_col = f'hcondever{i}'\n",
    "        hcondnew_col = f'hcondnew{i}'\n",
    "        \n",
    "        # Initialize the condition column\n",
    "        df[condition_col] = 0\n",
    "        \n",
    "        for wave in waves:\n",
    "            wave_mask = df['wave'] == wave\n",
    "            \n",
    "            if wave == 'a':\n",
    "                if hcond_col in df.columns:\n",
    "                    df.loc[wave_mask & (df[hcond_col] == 1), condition_col] = 1\n",
    "                    df.loc[wave_mask & (df[hcond_col].isna()), condition_col] = np.nan\n",
    "            elif wave == 'b':  # Wave 2\n",
    "                if hcondn_col in df.columns:\n",
    "                    df.loc[wave_mask & (df[hcondn_col] == 1), condition_col] = 1\n",
    "                    df.loc[wave_mask & (df[hcondn_col].isna()) & (df[condition_col] != 1), condition_col] = np.nan\n",
    "            elif wave in 'cdefghi':  # Waves 3-9\n",
    "                if hcond_col in df.columns and hcondn_col in df.columns:\n",
    "                    df.loc[wave_mask & ((df[hcond_col] == 1) | (df[hcondn_col] == 1)), condition_col] = 1\n",
    "                    df.loc[wave_mask & (df[hcondn_col].isna()) & (df[hcond_col].isna()) & (df[condition_col] != 1), condition_col] = np.nan\n",
    "            elif wave == 'j':  # Wave 10\n",
    "                if hcond_col in df.columns and hcondever_col in df.columns:\n",
    "                    df.loc[wave_mask & ((df[hcond_col] == 1) | (df[hcondever_col] == 1)), condition_col] = 1\n",
    "                    df.loc[wave_mask & (df[hcondever_col].isna()) & (df[hcond_col].isna()) & (df[condition_col] != 1), condition_col] = np.nan\n",
    "            elif wave in 'klm':  # Waves 11-13\n",
    "                if hcond_col in df.columns and hcondnew_col in df.columns:\n",
    "                    df.loc[wave_mask & ((df[hcond_col] == 1) | (df[hcondnew_col] == 1)), condition_col] = 1\n",
    "                    df.loc[wave_mask & (df[hcondnew_col].isna()) & (df[hcond_col].isna()) & (df[condition_col] != 1), condition_col] = np.nan\n",
    "        \n",
    "        # Forward fill the condition\n",
    "        df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.fillna(method='ffill'))\n",
    "        \n",
    "        # Ensure that 1s are not overwritten\n",
    "        df[condition_col] = df.groupby('pidp')[condition_col].transform(lambda x: x.where(x != 1, 1))\n",
    "        \n",
    "        # Set to NaN if all previous values were NaN\n",
    "        df[condition_col] = df.groupby('pidp')[condition_col].transform(\n",
    "            lambda x: x.where(x.notna().cumsum() > 0, np.nan)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to create health condition columns\n",
    "df = create_health_condition_columns(df)\n",
    "\n",
    "# Sort the dataframe by pidp and wave\n",
    "df = df.sort_values(['pidp', 'wave'])\n",
    "\n",
    "# Save the updated dataset\n",
    "output_path = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_health_conditions_long_panel.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated dataset saved to {output_path}\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code reorganizes the death dataset into a long panel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long panel death data saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/death_data_long_panel.csv\n",
      "Total rows: 4444\n",
      "Unique pidps: 4444\n",
      "\n",
      "Sample of the data:\n",
      "       pidp wave  death\n",
      "0   3490445    l      1\n",
      "1  68006807    j      1\n",
      "2  68014287    l      1\n",
      "3  68020407    l      1\n",
      "4  68025847    j      1\n",
      "5  68034007    c      1\n",
      "6  68044891    e      1\n",
      "7  68048287    j      1\n",
      "8  68048291    c      1\n",
      "9  68060525    m      1\n",
      "\n",
      "No pidps with multiple death records found.\n",
      "\n",
      "Distribution of deaths by wave:\n",
      "wave\n",
      "b    276\n",
      "c    381\n",
      "d    457\n",
      "e    347\n",
      "f    351\n",
      "g    433\n",
      "h    432\n",
      "i    378\n",
      "j    366\n",
      "k    346\n",
      "l    337\n",
      "m    340\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset from the new CSV file\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/extracted_dcsedw_dv_with_deaths.csv')\n",
    "\n",
    "# Define waves, starting from 'b'\n",
    "waves = 'bcdefghijklm'\n",
    "\n",
    "# Remove rows that are all zero from 'b_death' to 'm_death'\n",
    "death_columns = [f'{wave}_death' for wave in waves]\n",
    "df = df[~(df[death_columns] == 0).all(axis=1)]\n",
    "\n",
    "# Melt the dataframe to long format\n",
    "df_long = pd.melt(df, id_vars=['pidp'], value_vars=death_columns, \n",
    "                  var_name='wave', value_name='death')\n",
    "\n",
    "# Extract wave letter from 'wave' column\n",
    "df_long['wave'] = df_long['wave'].str[0]\n",
    "\n",
    "# Remove rows where death is 0\n",
    "df_long = df_long[df_long['death'] != 0]\n",
    "\n",
    "# Sort by pidp and wave\n",
    "df_long = df_long.sort_values(['pidp', 'wave'])\n",
    "\n",
    "# Reset index\n",
    "df_long = df_long.reset_index(drop=True)\n",
    "\n",
    "# Save the long format death dataset\n",
    "output_path = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/death_data_long_panel.csv'\n",
    "df_long.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Long panel death data saved to {output_path}\")\n",
    "print(f\"Total rows: {len(df_long)}\")\n",
    "print(f\"Unique pidps: {df_long['pidp'].nunique()}\")\n",
    "print(\"\\nSample of the data:\")\n",
    "print(df_long.head(10))\n",
    "\n",
    "# Additional check for multiple death records\n",
    "multiple_deaths = df_long.groupby('pidp').size().sort_values(ascending=False)\n",
    "if (multiple_deaths > 1).any():\n",
    "    print(\"\\nWarning: Some pidps have multiple death records:\")\n",
    "    print(multiple_deaths[multiple_deaths > 1].head())\n",
    "else:\n",
    "    print(\"\\nNo pidps with multiple death records found.\")\n",
    "\n",
    "# Distribution of deaths by wave\n",
    "print(\"\\nDistribution of deaths by wave:\")\n",
    "print(df_long['wave'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of the Frailty Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assigned a frailty of 1 if that person has died, using a previously extracted dataframe with death and wave data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Frailty_Calculate Function does the following: \n",
    "- Checks if all health conditions are NaN across all waves for a pidp, if so, it sets the frailty to NaN for all waves of that pidp, otherwise, it calculates the frailty for each wave, treating NaN as 0.\n",
    "- We apply this function to each pidp group:\n",
    "We're calculating frailty wave by wave for each individual.\n",
    "If an individual has no health data across all waves, their frailty is set to NaN for all waves.\n",
    "For individuals with at least some health data, we calculate frailty for each wave, treating NaN values as 0 in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3789240596.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('pidp').apply(calculate_frailty).reset_index(drop=True)\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3789240596.py:42: FutureWarning: SeriesGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use Series.fillna instead\n",
      "  df['death'] = df.groupby('pidp')['death'].fillna(method='ffill')\n",
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_33083/3789240596.py:42: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['death'] = df.groupby('pidp')['death'].fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data with frailty measures saved to /Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/frailty_long_panel.csv\n",
      "\n",
      "Frailty Summary:\n",
      "count    524423.000000\n",
      "mean          0.028256\n",
      "std           0.059233\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.037037\n",
      "max           0.703704\n",
      "Name: frailty, dtype: float64\n",
      "\n",
      "Frailty distribution:\n",
      "frailty\n",
      "0.000000    0.693457\n",
      "0.037037    0.136939\n",
      "0.074074    0.063294\n",
      "0.111111    0.037613\n",
      "0.148148    0.024255\n",
      "0.185185    0.016185\n",
      "0.222222    0.011174\n",
      "0.259259    0.007092\n",
      "0.296296    0.004374\n",
      "0.333333    0.002483\n",
      "0.370370    0.001489\n",
      "0.407407    0.000820\n",
      "0.444444    0.000400\n",
      "0.481481    0.000215\n",
      "0.518519    0.000122\n",
      "0.555556    0.000051\n",
      "0.592593    0.000017\n",
      "0.629630    0.000008\n",
      "0.666667    0.000004\n",
      "0.703704    0.000006\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Number of pidps in death dataset not found in main dataset: 253\n",
      "Sample of missing pidps: [428893697, 750635011, 428893701, 1093406214, 353776133]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/ukhls_health_conditions_long_panel.csv')\n",
    "death_df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/death_data_long_panel.csv')\n",
    "\n",
    "# Define output path\n",
    "output_path = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/frailty_long_panel.csv'\n",
    "\n",
    "# Define waves and create wave_order\n",
    "waves = 'abcdefghijklm'\n",
    "wave_order = {wave: i for i, wave in enumerate(waves)}\n",
    "df['wave_order'] = df['wave'].map(wave_order)\n",
    "\n",
    "# Merge df and death_df based on 'pidp' and 'wave'\n",
    "df = df.merge(death_df[['pidp', 'wave', 'death']], on=['pidp', 'wave'], how='left')\n",
    "\n",
    "# Assign 0 to all non-one values in the death column\n",
    "df['death'] = df['death'].fillna(0).astype(int)\n",
    "df.loc[df['death'] != 1, 'death'] = 0\n",
    "\n",
    "# Calculate frailty wave by wave\n",
    "health_cols = [f'healthcond{i}' for i in range(1, 17)] + [f'disdif{i}' for i in range(1, 12)]\n",
    "\n",
    "def calculate_frailty(group):\n",
    "    # Check if all health conditions are NaN across all waves for this pidp\n",
    "    if group[health_cols].isna().all().all():\n",
    "        group['frailty'] = np.nan\n",
    "    else:\n",
    "        # Calculate frailty for each wave, treating NaN as 0\n",
    "        group['frailty'] = group[health_cols].fillna(0).sum(axis=1) / 27\n",
    "    return group\n",
    "\n",
    "# Apply the frailty calculation to each pidp group\n",
    "df = df.groupby('pidp').apply(calculate_frailty).reset_index(drop=True)\n",
    "\n",
    "# Sort the dataframe\n",
    "df = df.sort_values(['pidp', 'wave_order'])\n",
    "\n",
    "# Forward fill the death indicator within each pidp group\n",
    "df['death'] = df.groupby('pidp')['death'].fillna(method='ffill')\n",
    "\n",
    "# Set frailty to 1 for individuals from their death wave onwards\n",
    "df.loc[df['death'] == 1, 'frailty'] = 1.0\n",
    "\n",
    "# Drop temporary column\n",
    "df = df.drop('wave_order', axis=1)\n",
    "\n",
    "# Save the result\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nData with frailty measures saved to {output_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nFrailty Summary:\")\n",
    "print(df['frailty'].describe())\n",
    "\n",
    "print(\"\\nFrailty distribution:\")\n",
    "print(df['frailty'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Check for any pidps in death_df not in main df\n",
    "missing_pidps = set(death_df['pidp']) - set(df['pidp'])\n",
    "if missing_pidps:\n",
    "    print(f\"\\nNumber of pidps in death dataset not found in main dataset: {len(missing_pidps)}\")\n",
    "    print(\"Sample of missing pidps:\", list(missing_pidps)[:5])\n",
    "else:\n",
    "    print(\"\\nAll pidps from death dataset are present in main dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Frailty from age 20 and above\n",
    "We want a smoothed line of best fit rather than a percentile representation. We can use a **LOWESS** (Locally Weighted Scatterplot Smoothing) approach to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/frailty_long_panel.csv')\n",
    "\n",
    "# Filter for age > 20 and exclude NaN frailty values\n",
    "df_filtered = df[(df['age_dv'] > 20) & (~np.isnan(df['frailty']))]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.regplot(x='age_dv', y='frailty', data=df_filtered, scatter=True, \n",
    "            scatter_kws={'alpha': 0.1},  # This makes the scatter points semi-transparent\n",
    "            line_kws={'color': 'red'})\n",
    "\n",
    "plt.title('Frailty Index by Age (Above 20 Years, Excluding NaN)', fontsize=16)\n",
    "plt.xlabel('Age', fontsize=14)\n",
    "plt.ylabel('Frailty Index', fontsize=14)\n",
    "\n",
    "# Improve the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Plots/frailty_by_age_no_nan.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(df_filtered[['age_dv', 'frailty']].describe())\n",
    "\n",
    "# Print the number of excluded NaN values\n",
    "num_nan = df[df['age_dv'] > 20]['frailty'].isna().sum()\n",
    "print(f\"\\nNumber of NaN frailty values excluded for age > 20: {num_nan}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
