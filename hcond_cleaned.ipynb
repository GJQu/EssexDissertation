{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Condition using 'hcond' and 'hcondnew'\n",
    "### By Gavin Qu - May 23rd 2024\n",
    "#### Data Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tEncode hcond and new healthcond variables correctly \n",
    "-\tNote that individuals are asked about pre-existing health conditions on their first interview in the UKHLS – the hcond (i) variable, where i codes different conditions – and then asked whether they have developed new conditions in subsequent interviews – the hcondn(i) variable in waves 1-9 and hcondnew(i) in waves 10 onwards.\n",
    "-\thcond in wave 1 and new entrants for succeeding waves, hcondn in wave 1-9, hcondnew in wave 10-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, hcond1-19 has 1, 3-13 waves, and it's for new interviewees only. While hcondn1-19 have 2-9 waves asking the existing interviewees about newly devloped conditions, and hcondnew1-19 have wave 9-13 for the same questions. \n",
    "hcond21 and hcondnew21 only exist from wave 10-13, while hcondnew22 only exist from 10-13. \n",
    "\n",
    "'dcsedfl_dv' is death data, but it's onyl 50% accurate when it comes to health mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the long panel dataset you created has all the correct values, including the special codes like missing values, proxy, refusal, etc., you can use pandas to display the unique values for each variable. This way, you can verify that all expected values are present in the dataset.\n",
    "**Here's a script that:**\n",
    "- Loads the long panel dataset.\n",
    "- Displays the unique values for each variable.\n",
    "- Checks for the presence of the specified special codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in column 'pidp': [  68001367   68004087   68006127 ... 1644552890 1644675410 1649095330]\n",
      "Unique values in column 'wave': ['a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm']\n",
      "Unique values in column 'variable': ['a_hcond11' 'a_hcond13' 'a_hcond5' 'a_hcond7' 'a_hcond1' 'a_hcond15'\n",
      " 'a_hcond17' 'a_hcond9' 'a_hcond10' 'a_hcond14' 'a_hcond16' 'a_hcond12'\n",
      " 'a_hcond8' 'a_hcond3' 'a_hcond6' 'a_hcond2' 'a_hcond4' 'b_hcondn9'\n",
      " 'b_hcondn1' 'b_hcondn6' 'b_hcondn11' 'b_hcondn10' 'b_hcondn8' 'b_hcondn4'\n",
      " 'b_hcondn7' 'b_hcondn17' 'b_hcondn14' 'b_hcondn2' 'b_hcondn3' 'b_hcondn5'\n",
      " 'b_hcondn15' 'b_hcondn16' 'b_hcondn13' 'b_hcondn12' 'c_hcondn17'\n",
      " 'c_hcondn10' 'c_hcondn1' 'c_hcond11' 'c_hcond14' 'c_hcond3' 'c_hcondn13'\n",
      " 'c_hcondn15' 'c_hcond17' 'c_hcondn4' 'c_hcond1' 'c_hcond7' 'c_hcondn3'\n",
      " 'c_hcondn11' 'c_hcondn14' 'c_hcondn5' 'c_hcond5' 'c_hcond4' 'c_hcond9'\n",
      " 'c_hcondn12' 'c_hcondn2' 'c_hcond16' 'c_hcondn16' 'c_hcond6' 'c_hcond13'\n",
      " 'c_hcond15' 'c_hcond8' 'c_hcond2' 'c_hcondn9' 'c_hcond12' 'c_hcondn7'\n",
      " 'c_hcondn8' 'c_hcond10' 'c_hcondn6' 'd_hcondn8' 'd_hcondn10' 'd_hcond6'\n",
      " 'd_hcondn7' 'd_hcondn5' 'd_hcondn4' 'd_hcond13' 'd_hcond7' 'd_hcond3'\n",
      " 'd_hcond2' 'd_hcond15' 'd_hcond5' 'd_hcondn16' 'd_hcond14' 'd_hcondn17'\n",
      " 'd_hcondn3' 'd_hcond17' 'd_hcondn9' 'd_hcond1' 'd_hcondn15' 'd_hcondn12'\n",
      " 'd_hcondn2' 'd_hcond11' 'd_hcondn6' 'd_hcondn14' 'd_hcond8' 'd_hcond9'\n",
      " 'd_hcond12' 'd_hcondn13' 'd_hcondn11' 'd_hcond16' 'd_hcond4' 'd_hcond10'\n",
      " 'd_hcondn1' 'e_hcond9' 'e_hcondn8' 'e_hcondn4' 'e_hcondn6' 'e_hcondn1'\n",
      " 'e_hcondn2' 'e_hcondn3' 'e_hcondn14' 'e_hcond2' 'e_hcond8' 'e_hcond15'\n",
      " 'e_hcondn13' 'e_hcond7' 'e_hcond16' 'e_hcond13' 'e_hcond17' 'e_hcondn5'\n",
      " 'e_hcondn7' 'e_hcond11' 'e_hcondn11' 'e_hcond10' 'e_hcondn17' 'e_hcond12'\n",
      " 'e_hcond3' 'e_hcondn10' 'e_hcond4' 'e_hcondn15' 'e_hcond1' 'e_hcondn16'\n",
      " 'e_hcond14' 'e_hcond5' 'e_hcond6' 'e_hcondn12' 'e_hcondn9' 'f_hcond2'\n",
      " 'f_hcond11' 'f_hcondn12' 'f_hcond4' 'f_hcondn14' 'f_hcond3' 'f_hcond5'\n",
      " 'f_hcondn5' 'f_hcondn9' 'f_hcondn16' 'f_hcond10' 'f_hcondn2' 'f_hcondn10'\n",
      " 'f_hcond13' 'f_hcondn17' 'f_hcond1' 'f_hcondn7' 'f_hcond16' 'f_hcondn15'\n",
      " 'f_hcondn6' 'f_hcond8' 'f_hcondn18' 'f_hcond18' 'f_hcondn1' 'f_hcondn11'\n",
      " 'f_hcondn13' 'f_hcond6' 'f_hcondn3' 'f_hcond9' 'f_hcond17' 'f_hcondn4'\n",
      " 'f_hcondn8' 'f_hcond14' 'f_hcond7' 'f_hcond12' 'f_hcond15' 'g_hcond2'\n",
      " 'g_hcond19' 'g_hcondn3' 'g_hcond17' 'g_hcondn1' 'g_hcond4' 'g_hcondn13'\n",
      " 'g_hcond18' 'g_hcondn17' 'g_hcond14' 'g_hcond7' 'g_hcondn5' 'g_hcondn11'\n",
      " 'g_hcond11' 'g_hcond12' 'g_hcondn6' 'g_hcondn19' 'g_hcond1' 'g_hcond6'\n",
      " 'g_hcond5' 'g_hcond10' 'g_hcond13' 'g_hcondn8' 'g_hcond3' 'g_hcondn15'\n",
      " 'g_hcondn9' 'g_hcondn10' 'g_hcondn4' 'g_hcondn18' 'g_hcondn12'\n",
      " 'g_hcondn14' 'g_hcond8' 'g_hcond9' 'g_hcondn7' 'g_hcondn2' 'g_hcond15'\n",
      " 'g_hcond16' 'g_hcondn16' 'h_hcond10' 'h_hcondn8' 'h_hcondn12'\n",
      " 'h_hcondn19' 'h_hcond3' 'h_hcond7' 'h_hcond2' 'h_hcondn17' 'h_hcond8'\n",
      " 'h_hcondn9' 'h_hcond12' 'h_hcondn5' 'h_hcondn16' 'h_hcondn4' 'h_hcond18'\n",
      " 'h_hcondn7' 'h_hcondn10' 'h_hcond16' 'h_hcond4' 'h_hcond9' 'h_hcond11'\n",
      " 'h_hcondn14' 'h_hcondn3' 'h_hcond14' 'h_hcondn13' 'h_hcond15' 'h_hcondn6'\n",
      " 'h_hcond6' 'h_hcondn2' 'h_hcond19' 'h_hcondn11' 'h_hcond13' 'h_hcondn18'\n",
      " 'h_hcondn15' 'h_hcond5' 'h_hcond1' 'h_hcondn1' 'h_hcond17' 'i_hcond9'\n",
      " 'i_hcondn17' 'i_hcond3' 'i_hcondn10' 'i_hcondn11' 'i_hcondn18'\n",
      " 'i_hcondn13' 'i_hcond6' 'i_hcond13' 'i_hcondn1' 'i_hcond19' 'i_hcond12'\n",
      " 'i_hcond5' 'i_hcondn6' 'i_hcondn14' 'i_hcondn15' 'i_hcond15' 'i_hcondn19'\n",
      " 'i_hcond7' 'i_hcondn5' 'i_hcondn8' 'i_hcond18' 'i_hcondn3' 'i_hcond1'\n",
      " 'i_hcond4' 'i_hcond11' 'i_hcondn7' 'i_hcondn2' 'i_hcond10' 'i_hcondn4'\n",
      " 'i_hcond14' 'i_hcondn16' 'i_hcondn12' 'i_hcond16' 'i_hcondn9' 'i_hcond2'\n",
      " 'i_hcond8' 'i_hcond17' 'j_hcond5' 'j_hcond18' 'j_hcond19' 'j_hcond16'\n",
      " 'j_hcond12' 'j_hcond8' 'j_hcond21' 'j_hcond15' 'j_hcond2' 'j_hcond1'\n",
      " 'j_hcond13' 'j_hcond3' 'j_hcond11' 'j_hcond4' 'j_hcond7' 'j_hcond6'\n",
      " 'j_hcond10' 'j_hcond22' 'j_hcond14' 'k_hcond12' 'k_hcondnew2' 'k_hcond19'\n",
      " 'k_hcond15' 'k_hcondnew22' 'k_hcondnew10' 'k_hcondnew11' 'k_hcondnew14'\n",
      " 'k_hcond4' 'k_hcond13' 'k_hcond5' 'k_hcondnew6' 'k_hcondnew15'\n",
      " 'k_hcondnew16' 'k_hcond7' 'k_hcondnew8' 'k_hcond3' 'k_hcondnew7'\n",
      " 'k_hcond10' 'k_hcondnew12' 'k_hcondnew19' 'k_hcondnew4' 'k_hcond22'\n",
      " 'k_hcond2' 'k_hcond14' 'k_hcondnew1' 'k_hcondnew13' 'k_hcondnew5'\n",
      " 'k_hcond18' 'k_hcond6' 'k_hcond16' 'k_hcond1' 'k_hcondnew3' 'k_hcond8'\n",
      " 'k_hcondnew21' 'k_hcond11' 'k_hcond21' 'l_hcond18' 'l_hcond19'\n",
      " 'l_hcond10' 'l_hcondnew6' 'l_hcond3' 'l_hcond16' 'l_hcondnew8'\n",
      " 'l_hcondnew19' 'l_hcondnew1' 'l_hcondnew22' 'l_hcond5' 'l_hcond7'\n",
      " 'l_hcond2' 'l_hcondnew15' 'l_hcondnew16' 'l_hcondnew11' 'l_hcondnew13'\n",
      " 'l_hcond21' 'l_hcond4' 'l_hcond11' 'l_hcondnew5' 'l_hcond15'\n",
      " 'l_hcondnew10' 'l_hcondnew14' 'l_hcond12' 'l_hcondnew3' 'l_hcondnew7'\n",
      " 'l_hcond1' 'l_hcond22' 'l_hcondnew12' 'l_hcond8' 'l_hcondnew4'\n",
      " 'l_hcond13' 'l_hcond6' 'l_hcond14' 'l_hcondnew21' 'l_hcondnew2'\n",
      " 'm_hcond3' 'm_hcondnew7' 'm_hcondnew1' 'm_hcondnew14' 'm_hcond1'\n",
      " 'm_hcond4' 'm_hcondnew22' 'm_hcond10' 'm_hcondnew19' 'm_hcondnew12'\n",
      " 'm_hcond6' 'm_hcondnew10' 'm_hcond16' 'm_hcond19' 'm_hcond14' 'm_hcond11'\n",
      " 'm_hcond7' 'm_hcond13' 'm_hcondnew3' 'm_hcondnew2' 'm_hcondnew4'\n",
      " 'm_hcondnew11' 'm_hcondnew16' 'm_hcond8' 'm_hcondnew5' 'm_hcondnew13'\n",
      " 'm_hcondnew6' 'm_hcond21' 'm_hcond18' 'm_hcond12' 'm_hcond15'\n",
      " 'm_hcondnew8' 'm_hcondnew15' 'm_hcond2' 'm_hcond5' 'm_hcondnew21']\n",
      "Unique values in column 'value': [  0.   1.  -7.  -2.  -9.  -1.  nan  -8. -10.]\n",
      "  missing (-9) is present in column 'value'\n",
      "  proxy (-7) is present in column 'value'\n",
      "  refusal (-2) is present in column 'value'\n",
      "  don't know (-1) is present in column 'value'\n",
      "  not mentioned (0) is present in column 'value'\n",
      "  mentioned (1) is present in column 'value'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the long panel data from the Stata file\n",
    "long_panel_data_path = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data/long_panel_ukhls_hcond_data.dta'\n",
    "long_panel_data = pd.read_stata(long_panel_data_path)\n",
    "\n",
    "# Define the special codes to check\n",
    "special_codes = {\n",
    "    'missing': -9,\n",
    "    'proxy': -7,\n",
    "    'refusal': -2,\n",
    "    'don\\'t know': -1,\n",
    "    'not mentioned': 0,\n",
    "    'mentioned': 1\n",
    "}\n",
    "\n",
    "# Function to check special codes in each variable\n",
    "def check_special_codes(df, special_codes):\n",
    "    for column in df.columns:\n",
    "        if column not in ['pidp', 'wave', 'variable']:\n",
    "            unique_values = df[column].unique()\n",
    "            print(f\"Unique values in column '{column}': {unique_values}\")\n",
    "            for code_name, code_value in special_codes.items():\n",
    "                if code_value in unique_values:\n",
    "                    print(f\"  {code_name} ({code_value}) is present in column '{column}'\")\n",
    "                else:\n",
    "                    print(f\"  {code_name} ({code_value}) is NOT present in column '{column}'\")\n",
    "        else:\n",
    "            unique_values = df[column].unique()\n",
    "            print(f\"Unique values in column '{column}': {unique_values}\")\n",
    "\n",
    "# Check for special codes in the long panel dataset\n",
    "check_special_codes(long_panel_data, special_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New approach to include the disdif and hcond along with age and death value in the regular panel format: \n",
    "1. Load the wave data and extract age variables.\n",
    "2. Combine data from all waves.\n",
    "3. Load and merge death information.\n",
    "4. Clean the combined data.\n",
    "5. Save the cleaned combined data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls/a_indresp.dta\n",
      "Available columns in a: {'a_hcond17', 'a_disdif8', 'a_disdif5', 'a_hcond2', 'a_hcond6', 'a_hcond4', 'a_disdif1', 'a_hcond14', 'a_disdif3', 'a_disdif10', 'a_age_dv', 'a_disdif2', 'a_hcond8', 'a_hcond11', 'a_hcond1', 'a_disdif9', 'a_disdif6', 'a_disdif7', 'a_hcond13', 'a_hcond10', 'a_hcond9', 'a_hcond3', 'a_hcond5', 'a_hcond12', 'a_hcond16', 'a_disdif11', 'pidp', 'a_disdif4', 'a_hcond15', 'a_hcond7'}\n",
      "Loading data from /Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls/b_indresp.dta\n",
      "Available columns in b: {'b_disdif5', 'b_hcondn5', 'b_disdif4', 'b_disdif9', 'b_hcondn10', 'b_hcondn8', 'b_hcondn1', 'b_hcondn13', 'b_hcondn9', 'b_disdif1', 'b_disdif6', 'b_hcondn2', 'b_hcondn11', 'b_hcondn16', 'b_hcondn17', 'b_hcondn6', 'b_disdif7', 'b_age_dv', 'b_disdif3', 'b_disdif11', 'b_disdif10', 'b_hcondn15', 'b_hcondn14', 'b_disdif8', 'b_disdif2', 'b_hcondn12', 'b_hcondn3', 'pidp', 'b_hcondn4', 'b_hcondn7'}\n",
      "Loading data from /Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls/c_indresp.dta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n2/8hz3y3r90rj63gkzgrl1hwg40000gn/T/ipykernel_94419/3811719406.py:33: UnicodeWarning: \n",
      "One or more strings in the dta file could not be decoded using utf-8, and\n",
      "so the fallback encoding of latin-1 is being used.  This can happen when a file\n",
      "has been incorrectly encoded by Stata or some other software. You should verify\n",
      "the string values returned are correct.\n",
      "  wave_data = pd.read_stata(file_path, convert_categoricals=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in c: {'c_disdif3', 'c_hcondn11', 'c_hcond2', 'c_hcond15', 'c_hcond6', 'c_hcond16', 'c_disdif5', 'c_disdif4', 'c_hcondn1', 'c_hcond7', 'c_disdif6', 'c_hcondn7', 'c_hcond4', 'c_hcond13', 'c_disdif9', 'c_disdif11', 'c_hcond9', 'c_hcondn17', 'c_disdif7', 'c_hcond12', 'c_hcondn6', 'c_hcondn15', 'c_hcondn3', 'c_hcond14', 'c_hcondn5', 'c_hcond11', 'c_hcondn12', 'c_hcond1', 'c_hcond10', 'c_age_dv', 'c_disdif1', 'c_hcondn16', 'c_disdif2', 'c_hcond3', 'c_hcondn10', 'c_hcondn4', 'c_hcond5', 'c_disdif10', 'c_hcondn13', 'c_hcondn14', 'c_hcondn8', 'pidp', 'c_hcond8', 'c_hcondn2', 'c_disdif8', 'c_hcondn9', 'c_hcond17'}\n",
      "Loading data from /Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls/d_indresp.dta\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base directory containing the data files\n",
    "base_dir = '/Users/gavinqu/Desktop/School/Dissertation/UKDA-6614-stata/stata/stata13_se/ukhls'\n",
    "\n",
    "# List of base variable names to extract, including 'pidp'\n",
    "base_hcond_variables = [\n",
    "    'pidp', 'hcond1', 'hcond2', 'hcond3', 'hcond4', 'hcond5', 'hcond6', 'hcond7',\n",
    "    'hcond8', 'hcond9', 'hcond10', 'hcond11', 'hcond12', 'hcond13', 'hcond14',\n",
    "    'hcond15', 'hcond16', 'hcond17', 'hcond21', 'hcond22',\n",
    "    'hcondn1', 'hcondn2', 'hcondn3', 'hcondn4', 'hcondn5', 'hcondn6', 'hcondn7',\n",
    "    'hcondn8', 'hcondn9', 'hcondn10', 'hcondn11', 'hcondn12', 'hcondn13', 'hcondn14',\n",
    "    'hcondn15', 'hcondn16', 'hcondn17', 'hcondnew1', 'hcondnew2',\n",
    "    'hcondnew3', 'hcondnew4', 'hcondnew5', 'hcondnew6', 'hcondnew7', 'hcondnew8',\n",
    "    'hcondnew10', 'hcondnew11', 'hcondnew12', 'hcondnew13', 'hcondnew14', 'hcondnew15',\n",
    "    'hcondnew16'\n",
    "]\n",
    "\n",
    "base_disdif_variables = [f'disdif{i}' for i in range(1, 12)]\n",
    "\n",
    "# Combine all variable names to extract\n",
    "base_variables = ['pidp'] + base_hcond_variables[1:] + base_disdif_variables\n",
    "\n",
    "# Wave prefixes from 'a' to 'm'\n",
    "wave_prefixes = [chr(i) for i in range(ord('a'), ord('n'))]\n",
    "\n",
    "# Function to load and filter wave data\n",
    "def load_wave_data(wave_prefix, base_dir, base_variables):\n",
    "    file_path = os.path.join(base_dir, f'{wave_prefix}_indresp.dta')\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading data from {file_path}\")\n",
    "        wave_data = pd.read_stata(file_path, convert_categoricals=False)\n",
    "        \n",
    "        # Construct the actual variable names for the current wave\n",
    "        wave_variables = [f'{wave_prefix}_{var}' if var != 'pidp' else var for var in base_variables]\n",
    "        age_column = f'{wave_prefix}_age_dv'\n",
    "        wave_variables.append(age_column)\n",
    "        \n",
    "        # Find the intersection of desired variables and available columns\n",
    "        available_columns = set(wave_variables).intersection(wave_data.columns)\n",
    "        print(f\"Available columns in {wave_prefix}: {available_columns}\")\n",
    "        \n",
    "        # Select only the available columns\n",
    "        if available_columns:\n",
    "            selected_data = wave_data[list(available_columns)].copy()\n",
    "            selected_data['wave'] = wave_prefix\n",
    "            return selected_data\n",
    "    return None\n",
    "\n",
    "# List to store data from each wave\n",
    "all_waves_data = []\n",
    "\n",
    "# Loop through wave prefixes\n",
    "for prefix in wave_prefixes:\n",
    "    wave_data = load_wave_data(prefix, base_dir, base_variables)\n",
    "    if wave_data is not None:\n",
    "        all_waves_data.append(wave_data)\n",
    "\n",
    "# Combine all waves into a single DataFrame\n",
    "if all_waves_data:\n",
    "    combined_data = pd.concat(all_waves_data, ignore_index=True)\n",
    "\n",
    "    # Load the death information from xhhrel.dta\n",
    "    death_file_path = os.path.join(base_dir, 'xhhrel.dta')\n",
    "    death_data = pd.read_stata(death_file_path, convert_categoricals=False)\n",
    "\n",
    "    # Select the pidp and death column\n",
    "    death_data = death_data[['pidp', 'dcsedfl_dv']]\n",
    "\n",
    "    # Merge the death information with the combined data\n",
    "    combined_data = combined_data.merge(death_data, on='pidp', how='left')\n",
    "\n",
    "    # List of columns to clean (excluding 'pidp', 'wave', 'dcsedfl_dv', and age columns)\n",
    "    age_columns = [f'{prefix}_age_dv' for prefix in wave_prefixes]\n",
    "    columns_to_clean = [col for col in combined_data.columns if col not in ['pidp', 'wave', 'dcsedfl_dv'] + age_columns]\n",
    "\n",
    "    # Clean the data: treat everything that's not a 1 as 0\n",
    "    def clean_data(df, columns):\n",
    "        df_cleaned = df.copy()\n",
    "        for col in columns:\n",
    "            df_cleaned[col] = df_cleaned[col].apply(lambda x: 1 if x == 1 else 0)\n",
    "        return df_cleaned\n",
    "\n",
    "    # Apply the cleaning function\n",
    "    cleaned_combined_data = clean_data(combined_data, columns_to_clean)\n",
    "\n",
    "    # Display the first few rows of the cleaned combined data\n",
    "    print(\"Cleaned Combined Data Head:\")\n",
    "    print(cleaned_combined_data.head())\n",
    "\n",
    "    # Save the cleaned combined data to a new Stata file\n",
    "    output_dir = '/Users/gavinqu/Desktop/School/Dissertation/EssexDissertation/Data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cleaned_combined_data_path = os.path.join(output_dir, 'cleaned_combined_ukhls_hcond_disdif_death_data.dta')\n",
    "    cleaned_combined_data.to_stata(cleaned_combined_data_path, write_index=False)\n",
    "\n",
    "    print(f\"Cleaned combined data saved to {cleaned_combined_data_path}\")\n",
    "else:\n",
    "    print(\"No data was loaded. Please check the file paths and variable names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the frailty index using the cleaned data. Here’s the step-by-step approach:\n",
    "\n",
    "1. Load the cleaned data: Load the cleaned combined data file.\n",
    "2. Filter individuals aged 52-53: Include only individuals aged 52-53 and until either death or last wave. \n",
    "3. Calculate the frailty index: Calculate the frailty index for each individual at each point in time.\n",
    "4. Assign a frailty index of 1 for deceased individuals: Set the frailty index to 1 for individuals who have died."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
